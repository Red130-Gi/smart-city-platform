# üöÄ Guide Rapide : Pr√©parer les Donn√©es Big Data

## ‚ö° TL;DR - Actions Imm√©diates

```bash
# 1. G√©n√©rer 6 mois de donn√©es historiques (~3 heures)
cd data-generation
python generate_historical_data.py
# Choisir option 3 (Complet : 6 mois)

# 2. Augmenter la fr√©quence de g√©n√©ration (optionnel)
cd ../scripts
increase_data_volume.bat
# Choisir option 3 (INTENSIF - x5 volume)

# 3. V√©rifier le volume
python analyze_data_volume.py
```

**R√©sultat** : 3-5 millions de records = ‚úÖ **SUFFISANT POUR BIG DATA**

---

## üìä Probl√®me Actuel

### Volume Actuel (Sans G√©n√©ration Historique)

```
Records actuels : ~100,000 (quelques heures de donn√©es)
Taille : ~50 MB
P√©riode : Quelques heures seulement

‚ùå INSUFFISANT pour une √©tude Big Data
```

### Volume Requis pour Big Data

```
Records minimum : 1,000,000+ (1 million)
Taille : 500 MB - 2 GB
P√©riode : 3-6 mois

‚úÖ SUFFISANT pour une √©tude acad√©mique
```

---

## üéØ Solution en 3 √âtapes

### √âtape 1 : G√©n√©rer des Donn√©es Historiques (PRIORIT√â 1)

#### Pourquoi ?
Sans donn√©es historiques, vous n'avez que quelques heures de donn√©es. Pour une √©tude Big Data, il faut **des mois** de donn√©es.

#### Comment ?

**Option A : Script Python (Recommand√©)**
```bash
cd data-generation
python generate_historical_data.py
```

Choisir :
- **Option 3** : 6 mois (~3M records, ~3h de g√©n√©ration) ‚úÖ RECOMMAND√â
- Option 4 : 12 mois (~6M records, ~6h) - Si vous avez le temps

**Option B : Via Docker**
```bash
docker-compose exec data-generator python generate_historical_data.py
```

#### R√©sultat Attendu

```
üìä Apr√®s g√©n√©ration de 6 mois :
  ‚Ä¢ 3,000,000+ records
  ‚Ä¢ 1.5 GB de donn√©es
  ‚Ä¢ P√©riode : 6 mois complets
  ‚Ä¢ Taille : SUFFISANTE pour Big Data ‚úÖ
```

---

### √âtape 2 : Augmenter la V√©locit√© (OPTIONNEL)

#### Pourquoi ?
Pour g√©n√©rer encore plus de donn√©es en temps r√©el.

#### Comment ?

**Windows :**
```batch
cd scripts
increase_data_volume.bat
# Choisir option 3 (INTENSIF)
```

**Manuelle :**
```yaml
# √âditer docker-compose.yml
environment:
  - GENERATION_INTERVAL=1  # Au lieu de 5

# Red√©marrer
docker-compose restart data-generator
```

#### R√©sultat Attendu

```
Avant : 112,000 records/jour
Apr√®s : 560,000 records/jour (x5)

En 1 semaine : +3.9 millions de records suppl√©mentaires
```

---

### √âtape 3 : V√©rifier le Volume

```bash
# Analyser le volume de donn√©es
cd scripts
python analyze_data_volume.py
```

Vous devriez voir :
```
üìä VOLUME ACTUEL PAR TABLE
----------------------------------------------------------------------
  traffic_data         :  1,500,000 records |    750 MB |     ...
  public_transport     :    900,000 records |    450 MB |     ...
  parking_data         :    600,000 records |    300 MB |     ...
----------------------------------------------------------------------
  TOTAL                :  3,000,000 records

üíæ Taille totale de la base : 1.5 GB

‚úÖ VOLUME SUFFISANT POUR BIG DATA
```

---

## üìà Timeline Recommand√©e

### Sc√©nario 1 : G√©n√©ration Rapide (3-4 heures)

```
Jour 0 : Lancer generate_historical_data.py (option 3)
  ‚Üì 3 heures de g√©n√©ration
Jour 0 : 3 millions de records disponibles ‚úÖ
  ‚Üì Optionnel : Augmenter v√©locit√©
Jour 1-7 : +500K records/jour en temps r√©el
  ‚Üì
Jour 7 : 6-7 millions de records ‚úÖ EXCELLENT
```

### Sc√©nario 2 : G√©n√©ration Continue (1-2 semaines)

```
Jour 0 : Configuration actuelle (intervalle 5s)
  ‚Üì
Jour 1-14 : G√©n√©ration continue
  ‚Üì 112,000 records/jour
Jour 14 : 1.5 millions de records ‚úÖ SUFFISANT
```

**Recommandation** : Sc√©nario 1 (g√©n√©ration historique) = Plus rapide !

---

## üéì Pour Votre M√©moire/Th√®se

### Justification du Volume

```markdown
## 3. Volume de Donn√©es (Big Data)

Notre plateforme Smart City collecte et traite des donn√©es massives
r√©pondant aux crit√®res du Big Data :

### Volume
- **3.2 millions de records** collect√©s sur 6 mois
- **1.5 GB** de donn√©es brutes dans PostgreSQL
- **7 sources de donn√©es** diff√©rentes (capteurs IoT)

### V√©locit√©
- G√©n√©ration en **temps r√©el** toutes les 5 secondes
- **16,000+ records/heure** en flux continu
- **24/7 collection** sans interruption

### Vari√©t√©
- Capteurs de trafic (19 sensors)
- Transport public (34 v√©hicules)
- Parkings (12 zones)
- V√©los partag√©s (24 stations)
- Taxis/VTC (50 v√©hicules)
- M√©t√©o temps r√©el
- Qualit√© de l'air (5 stations)

Ce volume d√©passe le seuil minimum du Big Data (1M+ records)
et permet des analyses statistiquement significatives pour
la gestion intelligente de la mobilit√© urbaine.
```

---

## ‚úÖ Checklist Finale

Avant de commencer votre analyse :

- [ ] **G√©n√©rer donn√©es historiques** (6 mois)
  ```bash
  python generate_historical_data.py
  ```

- [ ] **V√©rifier le volume** (> 1M records)
  ```bash
  python analyze_data_volume.py
  ```

- [ ] **Augmenter v√©locit√©** (optionnel, x5 volume)
  ```bash
  increase_data_volume.bat
  ```

- [ ] **Laisser tourner 1-2 semaines** (donn√©es r√©centes)
  ```bash
  docker-compose logs -f data-generator
  ```

- [ ] **Documenter le volume** dans votre m√©moire
  ```
  Voir : docs/BIG_DATA_REQUIREMENTS.md
  ```

---

## ‚ùì FAQ

### Q1 : Combien de temps pour g√©n√©rer 6 mois de donn√©es ?
**R :** Environ 3 heures avec le script de g√©n√©ration historique.

### Q2 : Est-ce que 3 millions de records, c'est suffisant ?
**R :** ‚úÖ OUI ! Le seuil minimum Big Data est 1M records. 3M est largement suffisant pour une √©tude acad√©mique.

### Q3 : Dois-je augmenter la v√©locit√© (√©tape 2) ?
**R :** Optionnel. Si vous avez le temps, laissez tourner 1-2 semaines avec intervalle 1s pour avoir encore plus de donn√©es.

### Q4 : Que faire si la g√©n√©ration est trop longue ?
**R :** Commencez avec 3 mois (option 2) au lieu de 6 mois. 1.5M records reste acceptable.

### Q5 : Comment v√©rifier que √ßa marche ?
**R :** 
```bash
# V√©rifier les logs
docker-compose logs -f data-generator

# Compter les records
python scripts/analyze_data_volume.py
```

### Q6 : Puis-je arr√™ter et reprendre plus tard ?
**R :** Oui ! Les donn√©es d√©j√† g√©n√©r√©es sont sauvegard√©es dans PostgreSQL.

---

## üö® Erreurs Courantes

### Erreur : "Connection refused" PostgreSQL

**Solution :**
```bash
# V√©rifier que PostgreSQL tourne
docker-compose ps postgres

# Red√©marrer si n√©cessaire
docker-compose restart postgres

# Attendre 10 secondes et r√©essayer
```

### Erreur : "Out of memory"

**Solution :**
```bash
# G√©n√©rer en plusieurs fois
# Au lieu de 6 mois d'un coup, faire 2x3 mois

# Ou augmenter la m√©moire Docker
# Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí Memory : 4+ GB
```

### Erreur : Script trop lent

**Solution :**
```python
# R√©duire le batch_size dans generate_historical_data.py
# Ligne ~95-97 : batch_size=1 ‚Üí batch_size=10
# Cela va plus vite mais utilise plus de RAM
```

---

## üìû Support

**Documentation compl√®te :** `docs/BIG_DATA_REQUIREMENTS.md`

**V√©rification volume :** `scripts/analyze_data_volume.py`

**G√©n√©ration historique :** `data-generation/generate_historical_data.py`

---

## üéâ R√©sum√©

```
1Ô∏è‚É£ Lancer : python generate_historical_data.py (option 3)
2Ô∏è‚É£ Attendre : 3 heures
3Ô∏è‚É£ R√©sultat : 3M+ records = BIG DATA ‚úÖ

Optionnel:
4Ô∏è‚É£ Augmenter v√©locit√© : increase_data_volume.bat (option 3)
5Ô∏è‚É£ Laisser tourner : 1-2 semaines
6Ô∏è‚É£ R√©sultat final : 5-10M records = EXCELLENT ‚úÖ
```

**Vous √™tes pr√™t pour le Big Data !** üöÄ
