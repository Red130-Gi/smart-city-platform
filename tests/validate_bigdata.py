"""
Script de validation Big Data - Apache Spark
Tests du pipeline de streaming et de traitement
"""

import subprocess
import time
import json
from datetime import datetime
import requests

def print_header(title):
    """Affiche un en-t√™te format√©"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70)

def check_spark_container():
    """V√©rifie si le container Spark est en cours d'ex√©cution"""
    print_header("üîç V√âRIFICATION CONTAINER SPARK")
    
    try:
        result = subprocess.run(
            ['docker', 'ps', '--filter', 'name=spark', '--format', '{{.Names}}'],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        containers = result.stdout.strip().split('\n')
        spark_containers = [c for c in containers if c and 'spark' in c.lower()]
        
        if spark_containers:
            print(f"‚úÖ Containers Spark trouv√©s: {len(spark_containers)}")
            for container in spark_containers:
                print(f"   - {container}")
            return True
        else:
            print("‚ùå Aucun container Spark trouv√©")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

def check_spark_logs():
    """Analyse les logs Spark"""
    print_header("üìã ANALYSE DES LOGS SPARK")
    
    try:
        # R√©cup√©rer les logs r√©cents
        result = subprocess.run(
            ['docker', 'logs', '--tail', '100', 'smart-city-spark'],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        logs = result.stdout + result.stderr
        
        # V√©rifier les indicateurs cl√©s
        indicators = {
            'Spark Context': 'sparkcontext' in logs.lower() or 'spark context' in logs.lower(),
            'Streaming': 'streaming' in logs.lower(),
            'Kafka': 'kafka' in logs.lower(),
            'Processing': 'processing' in logs.lower() or 'batch' in logs.lower(),
            'Errors': 'error' in logs.lower() or 'exception' in logs.lower()
        }
        
        print("\nIndicateurs trouv√©s:")
        for indicator, found in indicators.items():
            status = "‚úÖ" if found else ("‚ùå" if indicator != "Errors" else "‚úÖ")
            if indicator == "Errors":
                status = "‚ö†Ô∏è" if found else "‚úÖ"
            print(f"  {status} {indicator}: {'Oui' if found else 'Non'}")
        
        # Afficher les derni√®res lignes importantes
        print("\nDerni√®res activit√©s:")
        lines = logs.split('\n')
        important_lines = [
            line for line in lines[-20:] 
            if any(keyword in line.lower() for keyword in ['batch', 'processing', 'completed', 'started', 'streaming'])
        ]
        
        for line in important_lines[-5:]:
            print(f"  {line[:100]}")
        
        return not indicators['Errors']
        
    except subprocess.TimeoutExpired:
        print("‚ö†Ô∏è Timeout lors de la r√©cup√©ration des logs")
        return False
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

def check_kafka_topics():
    """V√©rifie les topics Kafka"""
    print_header("üì® V√âRIFICATION KAFKA TOPICS")
    
    try:
        result = subprocess.run(
            ['docker', 'exec', 'smart-city-kafka', 
             'kafka-topics', '--list', '--bootstrap-server', 'localhost:9092'],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            topics = result.stdout.strip().split('\n')
            topics = [t for t in topics if t]
            
            print(f"‚úÖ Topics Kafka trouv√©s: {len(topics)}")
            for topic in topics:
                print(f"   - {topic}")
            
            # V√©rifier les topics essentiels
            essential_topics = ['traffic-events', 'predictions', 'anomalies']
            missing_topics = [t for t in essential_topics if t not in topics]
            
            if missing_topics:
                print(f"\n‚ö†Ô∏è Topics manquants: {', '.join(missing_topics)}")
            else:
                print("\n‚úÖ Tous les topics essentiels sont pr√©sents")
            
            return len(topics) > 0
        else:
            print(f"‚ùå Erreur Kafka: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

def check_kafka_messages():
    """V√©rifie les messages dans les topics Kafka"""
    print_header("üìä V√âRIFICATION MESSAGES KAFKA")
    
    topics_to_check = ['traffic-events', 'predictions']
    
    for topic in topics_to_check:
        try:
            print(f"\nTopic: {topic}")
            result = subprocess.run(
                ['docker', 'exec', 'smart-city-kafka',
                 'kafka-console-consumer', '--bootstrap-server', 'localhost:9092',
                 '--topic', topic, '--max-messages', '5', '--timeout-ms', '5000'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.stdout.strip():
                messages = result.stdout.strip().split('\n')
                print(f"  ‚úÖ {len(messages)} messages r√©cents")
                
                # Afficher un exemple de message
                if messages:
                    try:
                        sample = json.loads(messages[0])
                        print(f"  üìÑ Exemple: {json.dumps(sample, indent=2)[:200]}...")
                    except:
                        print(f"  üìÑ Exemple: {messages[0][:100]}...")
            else:
                print(f"  ‚ö†Ô∏è Aucun message r√©cent")
                
        except subprocess.TimeoutExpired:
            print(f"  ‚ö†Ô∏è Timeout - topic peut √™tre vide")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erreur: {e}")

def check_streaming_jobs():
    """V√©rifie les jobs Spark Streaming"""
    print_header("‚ö° V√âRIFICATION SPARK STREAMING JOBS")
    
    try:
        # Essayer d'acc√©der √† l'UI Spark
        spark_ui_url = "http://localhost:4040/api/v1/applications"
        
        try:
            response = requests.get(spark_ui_url, timeout=5)
            
            if response.status_code == 200:
                apps = response.json()
                print(f"‚úÖ Applications Spark actives: {len(apps)}")
                
                for app in apps:
                    print(f"\n  Application: {app.get('name', 'Unknown')}")
                    print(f"    ID: {app.get('id', 'N/A')}")
                    print(f"    Statut: {app.get('attempts', [{}])[0].get('completed', False)}")
                
                return True
            else:
                print("‚ö†Ô∏è Spark UI non accessible (normal si pas de jobs actifs)")
                return False
                
        except requests.ConnectionError:
            print("‚ö†Ô∏è Spark UI non accessible sur le port 4040")
            print("   (Normal si aucun job n'est en cours)")
            return False
            
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de v√©rifier les jobs: {e}")
        return False

def check_data_processing():
    """V√©rifie que les donn√©es sont trait√©es"""
    print_header("üîÑ V√âRIFICATION TRAITEMENT DES DONN√âES")
    
    try:
        # V√©rifier dans PostgreSQL si les donn√©es sont r√©centes
        result = subprocess.run(
            ['docker', 'exec', 'smart-city-postgres',
             'psql', '-U', 'smartcity', '-d', 'smartcitydb',
             '-c', "SELECT COUNT(*) FROM traffic_data WHERE timestamp > NOW() - INTERVAL '5 minutes'"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            # Extraire le nombre
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip().isdigit():
                    count = int(line.strip())
                    print(f"‚úÖ Donn√©es r√©centes (5 derni√®res minutes): {count} enregistrements")
                    
                    if count > 0:
                        print("‚úÖ Le pipeline traite activement les donn√©es")
                        return True
                    else:
                        print("‚ö†Ô∏è Aucune donn√©e r√©cente d√©tect√©e")
                        return False
        else:
            print("‚ö†Ô∏è Impossible de v√©rifier les donn√©es r√©centes")
            return False
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur: {e}")
        return False

def check_spark_metrics():
    """V√©rifie les m√©triques Spark"""
    print_header("üìà M√âTRIQUES SPARK")
    
    try:
        # R√©cup√©rer les stats du container
        result = subprocess.run(
            ['docker', 'stats', '--no-stream', '--format',
             '{{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}',
             'smart-city-spark'],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0 and result.stdout.strip():
            stats = result.stdout.strip().split('\t')
            if len(stats) >= 3:
                print(f"Container: {stats[0]}")
                print(f"  CPU: {stats[1]}")
                print(f"  M√©moire: {stats[2]}")
                
                # V√©rifier si le CPU est actif
                cpu_val = float(stats[1].replace('%', ''))
                if cpu_val > 0.5:
                    print("‚úÖ Spark est actif (CPU > 0.5%)")
                    return True
                else:
                    print("‚ö†Ô∏è Spark semble inactif (CPU tr√®s faible)")
                    return False
        else:
            print("‚ö†Ô∏è Impossible de r√©cup√©rer les m√©triques")
            return False
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur: {e}")
        return False

def generate_report(results):
    """G√©n√®re un rapport de validation"""
    print_header("üìä RAPPORT DE VALIDATION BIG DATA")
    
    total_checks = len(results)
    passed_checks = sum(1 for r in results.values() if r)
    success_rate = (passed_checks / total_checks * 100) if total_checks > 0 else 0
    
    print(f"\nTests ex√©cut√©s: {total_checks}")
    print(f"Tests r√©ussis: {passed_checks}")
    print(f"Taux de r√©ussite: {success_rate:.1f}%")
    
    print("\nD√©tails:")
    for check_name, result in results.items():
        status = "‚úÖ" if result else "‚ùå"
        print(f"  {status} {check_name}")
    
    print("\n" + "="*70)
    
    if success_rate >= 80:
        print("‚úÖ PLATEFORME BIG DATA OP√âRATIONNELLE")
    elif success_rate >= 50:
        print("‚ö†Ô∏è PLATEFORME BIG DATA PARTIELLEMENT OP√âRATIONNELLE")
    else:
        print("‚ùå PROBL√àMES CRITIQUES D√âTECT√âS")
    
    print("="*70)
    
    # Sauvegarder le rapport
    report = {
        'timestamp': datetime.now().isoformat(),
        'total_checks': total_checks,
        'passed_checks': passed_checks,
        'success_rate': success_rate,
        'details': results
    }
    
    try:
        with open('docs/BIGDATA_VALIDATION_REPORT.json', 'w') as f:
            json.dump(report, f, indent=2)
        print("\nüíæ Rapport sauvegard√©: docs/BIGDATA_VALIDATION_REPORT.json")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Erreur sauvegarde: {e}")

def main():
    """Fonction principale"""
    print("\n" + "="*70)
    print("  üß™ VALIDATION BIG DATA - APACHE SPARK")
    print("="*70)
    print(f"  Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # Ex√©cuter les v√©rifications
    results = {
        'Spark Container': check_spark_container(),
        'Spark Logs': check_spark_logs(),
        'Kafka Topics': check_kafka_topics(),
        'Data Processing': check_data_processing(),
        'Spark Metrics': check_spark_metrics()
    }
    
    # V√©rifications suppl√©mentaires
    check_kafka_messages()
    check_streaming_jobs()
    
    # G√©n√©rer le rapport
    generate_report(results)

if __name__ == "__main__":
    main()
